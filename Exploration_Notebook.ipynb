{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. Importing Necessary Packages\n",
    "2. Shipping Address Data\n",
    "    - Data Loading and Initial Inspection\n",
    "    - Null and Invalid ID Filtering\n",
    "    - Duplicate Record Handling\n",
    "    - Postal Code Validation and Cleaning\n",
    "    - Date Field Validation and Correction\n",
    "    - Customer ID Validation and Cleaning\n",
    "3. Product Data\n",
    "    - Loading and Transforming Product Data\n",
    "4. Region Data\n",
    "    - Loading Region Data\n",
    "5. Invoice Data\n",
    "    - Loading Invoice Data\n",
    "6. Customer Data\n",
    "    - Loading Customer Data\n",
    "7. Summary and Next Steps\n",
    "\n",
    "*Use the notebook's outline or search to quickly navigate to each section.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_json, from_json, map_entries, explode, col, isnan, isnull, when, lower, regexp_replace, coalesce, to_date, lit, regexp_extract, concat_ws, trim\n",
    "from pyspark.sql.types import MapType, StringType\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop any existing Spark session to avoid conflicts\n",
    "if 'spark' in globals():\n",
    "    spark.stop()\n",
    "    \n",
    "# Create a new Spark session with specific configurations  \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExplorationApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.15.0\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "\n",
    "### Shipping Address Data: Loading, Analysis, and Cleaning Approach\n",
    "\n",
    "The shipping address dataset is first loaded and explored to uncover discrepancies and data quality issues. The cleaning process follows a systematic approach:\n",
    "\n",
    "1. Attribute Analysis: Each attribute (column) is examined to identify potential inconsistencies, invalid values, or formatting issues.\n",
    "2. Dirty Data Subset: A subset of the data containing only the problematic (\"dirty\") records is created for focused analysis.\n",
    "3. Cleaning Logic Development: Appropriate cleaning logic is developed and tested on the dirty subset to ensure it addresses the identified issues.\n",
    "4. Validation: The cleaning logic is validated on the subset to confirm it produces the expected results.\n",
    "5. Full DataFrame Application: Once validated, the cleaning logic is applied to the entire dataset to standardize and correct all records. \n",
    "\n",
    "\n",
    "This iterative process ensures that data cleaning steps are robust, targeted, and do not introduce unintended side effects. Each major cleaning step is documented with findings and rationale for transparency and reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading shipping address data\n",
    "shipping_address_file_path = \"data/shippuingaddress_20240521.csv.csv\"\n",
    "address_rdd = spark.sparkContext.textFile(shipping_address_file_path)\n",
    "\n",
    "# Skip the first 6 rows using zipWithIndex to get zero-based indices\n",
    "shipping_address_rdd = address_rdd.zipWithIndex() \\\n",
    "    .filter(lambda x: x[1] >= 6) \\\n",
    "    .map(lambda x: x[0])\n",
    "\n",
    "shipping_address_df = spark.read.options(header='true', inferSchema='true').csv(shipping_address_rdd)\n",
    "\n",
    "shipping_address_df.printSchema()\n",
    "shipping_address_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the 'id' column\n",
    "shipping_address_df.filter(col(\"id\").isNull()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'id' is null\n",
    "shipping_address_df = shipping_address_df.dropna(subset=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find ids that are NOT all digits\n",
    "invalid_id_df = shipping_address_df.filter(~col(\"id\").rlike(\"^[0-9]+$\"))\n",
    "invalid_id_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows where 'id' is all digits\n",
    "shipping_address_df = shipping_address_df.filter(col(\"id\").rlike(\"^[0-9]+$\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate ids\n",
    "duplicate_ids = shipping_address_df.groupBy(\"id\").count().filter(col(\"count\") > 1).select(\"id\")\n",
    "duplicate_records = shipping_address_df.join(duplicate_ids, on=\"id\", how=\"inner\")\n",
    "duplicate_records.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "##### Summary of Findings After Filtering for id duplicate records\n",
    "After careful analysis of the duplicate records, the following were identified: \n",
    "- For record ID 102553, the clean version is:\n",
    "   id=102553, customerid=LB-16795, city=Antananarivo.\n",
    "   This will serve as the canonical record among all variants of ID 102553\n",
    "- For record ID 102608  \n",
    "  Will retain one representative instance through systematic selection\n",
    "\n",
    " Production Implementation Rules\n",
    "1. **Complete Match Preservation**  \n",
    "   Only records matching exactly across all columns will be kept\n",
    "\n",
    "2. **Partial Duplicate Removal**  \n",
    "   All records that match on some but not all attributes will be dropped\n",
    "\n",
    "3. **Quality Assurance**  \n",
    "   This conservative approach ensures:\n",
    "   - Consistent data integrity\n",
    "   - Elimination of ambiguous records\n",
    "   - Reliable downstream processing\n",
    "\n",
    "**Technical Justification**\n",
    "\n",
    "The strategy prioritises data quality over completeness to:\n",
    "- Avoid propagation of inconsistent records\n",
    "- Maintain clean master data\n",
    "- Support reliable analytics and operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for id=102553 and city=Antananarivo\n",
    "canonical_102553 = shipping_address_df.filter((col(\"id\") == \"102553\") & (col(\"city\") == \"Antananarivo\") & (col(\"country\") == \"Madagascar\"))\n",
    "\n",
    "# Filter for id=102608 and keep only one record (e.g., the first)\n",
    "one_102608 = shipping_address_df.filter(col(\"id\") == \"102608\").limit(1)\n",
    "\n",
    "# Combine the results\n",
    "final_selection = canonical_102553.union(one_102608)\n",
    "\n",
    "# Remove all duplicate records from shipping_address_df\n",
    "shipping_address_no_dupes = shipping_address_df.exceptAll(duplicate_records)\n",
    "\n",
    "# Add back only the desired records\n",
    "shipping_address_df = shipping_address_no_dupes.union(final_selection)\n",
    "\n",
    "shipping_address_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for postal_code that is NOT 4 or 5 digits\n",
    "invalid_postal_code_df = shipping_address_df.filter(~col(\"postal_code\").rlike(\"^[0-9]{4,5}$\"))\n",
    "invalid_postal_code_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "##### Summary of Findings After Postal Code Filtering\n",
    "\n",
    "After filtering the shipping address data for invalid postal codes (i.e., those not matching a 4- or 5-digit numeric pattern), several key observations were made:\n",
    "\n",
    "- **Identification of Invalid Postal Codes:**  \n",
    "  - Rows with postal codes that do not conform to the expected 4- or 5-digit numeric format were isolated. This includes postal codes that are missing, contain non-numeric characters, or are formatted as dates (e.g., \"DD/MM/YYYY\").\n",
    "  - Postal code wrongly inserted in other columns\n",
    "\n",
    "- **Data Quality Issues:**  \n",
    "  The presence of non-standard postal codes highlights inconsistencies in the data entry process. Some records use date formats or other unexpected values in the postal code field, which can hinder downstream processing and analytics.\n",
    "\n",
    "- **Country and State Normalization:**  \n",
    "  To address ambiguity, additional logic was applied to create a new column, `country_new`, based on the following rules:\n",
    "  - If the normalized (lowercased and accent-removed) city and state are identical, the country value is retained.\n",
    "  - If the postal code does not match a date format, the postal code itself is used.\n",
    "  - Otherwise, the state value is used.\n",
    "\n",
    "- **Impact on Data Integrity:**  \n",
    "  This filtering and normalization process improves the reliability of the address data by:\n",
    "  - Flagging and isolating problematic records for further review or correction.\n",
    "  - Providing a more consistent and meaningful value in the `country_new` column, which can be used for subsequent analysis or reporting.\n",
    "  - `postalcode_new` column is created my scanning through other attributes for wrongly inserted value of postal code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize city and state: lowercase and remove accents\n",
    "city_norm = lower(regexp_replace(col(\"city\"), \"[áéíóúüñ]\", \"a\"))\n",
    "state_norm = lower(regexp_replace(col(\"state\"), \"[áéíóúüñ]\", \"a\"))\n",
    "\n",
    "invalid_postal_code_df = invalid_postal_code_df.withColumn(\n",
    "    \"country_new\",\n",
    "    when(city_norm == state_norm, col(\"country\"))\n",
    "    .when(~col(\"postal_code\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\"), col(\"postal_code\"))\n",
    "    .otherwise(col(\"state\"))\n",
    ")\n",
    "\n",
    "\n",
    "invalid_postal_code_df = invalid_postal_code_df.withColumn(\n",
    "    \"postalcode_new\",\n",
    "    when(col(\"postal_code\").rlike(\"^[0-9]{4,5}$\"), col(\"postal_code\"))\n",
    "    .otherwise(\n",
    "        coalesce(\n",
    "            when(col('city').rlike(\"^[0-9]{4,5}$\"), col('city')),\n",
    "            when(col('state').rlike(\"^[0-9]{4,5}$\"), col('state')),\n",
    "            when(col('country').rlike(\"^[0-9]{4,5}$\"), col('country')),\n",
    "            when(col('effstart').rlike(\"^[0-9]{4,5}$\"), col('effstart')),\n",
    "            when(col('effend').rlike(\"^[0-9]{4,5}$\"), col('effend')),\n",
    "            when(col('streetadd').rlike(\"^[0-9]{4,5}$\"), col('streetadd'))\n",
    "        )\n",
    "    )\n",
    ")\n",
    "invalid_postal_code_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the same logic for the entire DataFrame  \n",
    "city_norm = lower(regexp_replace(col(\"city\"), \"[áéíóúüñ]\", \"a\"))\n",
    "state_norm = lower(regexp_replace(col(\"state\"), \"[áéíóúüñ]\", \"a\"))\n",
    "\n",
    "shipping_address_df = shipping_address_df.withColumn(\n",
    "    \"country_new\",\n",
    "     when(col(\"postal_code\").rlike(\"^[0-9]{4,5}$\") | col(\"postal_code\").isNull(), col(\"country\"))\n",
    "     .otherwise(when(city_norm == state_norm, col(\"country\"))\n",
    "                .when(~col(\"postal_code\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\"), col(\"postal_code\"))\n",
    "                .otherwise(col(\"state\")))\n",
    ")\n",
    "\n",
    "shipping_address_df = shipping_address_df.withColumn(\n",
    "    \"postalcode_new\",\n",
    "    when(col(\"postal_code\").rlike(\"^[0-9]{4,5}$\"), col(\"postal_code\"))\n",
    "    .otherwise(\n",
    "        coalesce(\n",
    "            when(col('city').rlike(\"^[0-9]{4,5}$\"), col('city')),\n",
    "            when(col('state').rlike(\"^[0-9]{4,5}$\"), col('state')),\n",
    "            when(col('country').rlike(\"^[0-9]{4,5}$\"), col('country')),\n",
    "            when(col('effstart').rlike(\"^[0-9]{4,5}$\"), col('effstart')),\n",
    "            when(col('effend').rlike(\"^[0-9]{4,5}$\"), col('effend')),\n",
    "            when(col('streetadd').rlike(\"^[0-9]{4,5}$\"), col('streetadd'))\n",
    "        )\n",
    "    )\n",
    ")\n",
    "shipping_address_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for effstart  that is NOT in the required format\n",
    "invalid_effstart_df = shipping_address_df.filter(~col(\"effstart\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\"))\n",
    "invalid_effstart_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "##### Summary of Findings After Filtering effstart \n",
    "During the validation of the `effstart` column, it was discovered that only one record did not conform to the expected date format (`DD/MM/YYYY`). A closer inspection of this record revealed a pattern of misplacement among the date-related columns:\n",
    "\n",
    "- The value in `effstart` was not a valid date, as expected.\n",
    "- The value in `effend` was a valid date, but it appeared to be earlier than the value in `streetadd`, which was also a valid date.\n",
    "- This sequence suggested that the values for `effstart`, `effend`, and `streetadd` had been shifted or swapped.\n",
    "\n",
    "**Business Reasoning:**\n",
    "- The logical order for these columns should be:  \n",
    "  `effstart` (start date) < `effend` (end date) < `streetadd` (should not be a date, but in this case, it is).\n",
    "- The presence of a date in `streetadd` and the chronological order of the dates indicate that the data was likely misaligned during entry or import.\n",
    "- This conclusion is supported by the fact that only one record is affected, and the pattern of the values matches what would be expected if the columns were shifted right by one position.\n",
    "\n",
    "**Remediation Plan:**\n",
    "- The affected record will be corrected by shifting the values back to their appropriate columns.\n",
    "- `effstart` will be explicitly cast or converted to the correct data type to ensure consistency and prevent similar issues in downstream processing.\n",
    "\n",
    "**Impact:**\n",
    "- This targeted correction will restore data integrity for the affected record and reinforce the reliability of the date fields for subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up the effstart and effend columns\n",
    "invalid_effstart_df = invalid_effstart_df.withColumn(\n",
    "    \"effstart\",\n",
    "        when(~col(\"effstart\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\"), col(\"effend\")).otherwise(col(\"effstart\"))\n",
    ").withColumn(\n",
    "    \"effend\",\n",
    "    when(~col(\"effstart\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\"), col(\"streetadd\")).otherwise(col(\"effend\"))\n",
    ").withColumn(\n",
    "    \"streetadd\",\n",
    "     when(~col(\"effstart\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\"), lit(None)).otherwise(col(\"streetadd\"))\n",
    ")\n",
    "\n",
    "invalid_effstart_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing the logic on the original DataFrame\n",
    "shipping_address_df = shipping_address_df.withColumn(\n",
    "    \"effstart\",\n",
    "        when(~col(\"effstart\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\"), col(\"effend\")).otherwise(col(\"effstart\"))\n",
    ").withColumn(\n",
    "    \"effend\",\n",
    "    when(~col(\"effstart\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\"), col(\"streetadd\")).otherwise(col(\"effend\"))\n",
    ").withColumn(\n",
    "    \"streetadd\",\n",
    "     when(~col(\"effstart\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\"), lit(None)).otherwise(col(\"streetadd\"))\n",
    ")       \n",
    "\n",
    "shipping_address_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for customerid  that is NOT in the required format\n",
    "invalid_customerid_df = shipping_address_df.filter(~col(\"customerid\").rlike(\"^[A-Za-z]{2}-\\d+$\"))\n",
    "invalid_customerid_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "##### Summary of Findings After Customer ID Validation and Cleaning\n",
    "\n",
    "During the analysis of the `customerid` field, several records were identified that did not conform to the expected format (`AA-12345`, where `AA` is a two-letter code and the rest is a numeric identifier). The following issues and cleaning steps were observed and addressed:\n",
    "\n",
    "**Findings:**\n",
    "- Some records contained extra or malformed values appended to the `customerid` field, such as trailing text or misplaced data.\n",
    "- In certain cases, the value that should have been in another column (e.g., `city` or `streetadd`) was incorrectly appended to the `customerid`.\n",
    "- A few records had missing or null values in the `city` column, with the misplaced value only present in the malformed `customerid`.\n",
    "- Occasionally, the `effend` field did not contain a valid date, and the misplaced value needed to be reassigned.\n",
    "\n",
    "**Cleaning Logic:**\n",
    "- The valid portion of the `customerid` (matching the pattern `^[A-Za-z]{2}-\\d+$`) was extracted and retained in the `customerid` column.\n",
    "- Any value trailing the valid pattern was extracted as a separate string.\n",
    "- If the `city` column was null and a trailing value was present, this value was moved to the `city` column.\n",
    "- If the `city` was not null and the `effend` field did not match a date format, the trailing value was appended to the `effend` value and placed in the `streetadd` column.\n",
    "- The helper column used for extraction was dropped after reassignment.\n",
    "\n",
    "**Impact:**\n",
    "- This approach restored the integrity of the `customerid` field, ensuring all values now match the required format.\n",
    "- Misplaced data was reassigned to its appropriate column, reducing ambiguity and improving the overall quality of the dataset.\n",
    "- The cleaning logic was designed to be robust, handling multiple edge cases and minimizing data loss.\n",
    "\n",
    "This process ensures that the `customerid` field is reliable for downstream processing and that related address fields are as accurate as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the value that comes after the valid customerid pattern\n",
    "invalid_customerid_df = invalid_customerid_df.withColumn(\n",
    "    \"customerid_stripped\",\n",
    "    regexp_extract(col(\"customerid\"), r\"^[A-Za-z]{2}-\\d+(.*)$\", 1)\n",
    ")\n",
    "\n",
    "# If city is null, put the stripped value in city\n",
    "invalid_customerid_df = invalid_customerid_df.withColumn(\n",
    "    \"city\",\n",
    "    when(col(\"city\").isNull() & (col(\"customerid_stripped\") != \"\"), col(\"customerid_stripped\")).otherwise(col(\"city\"))\n",
    ")\n",
    "\n",
    "# If effend doesn't match date, append the stripped value to effend and insert in streetadd\n",
    "invalid_customerid_df = invalid_customerid_df.withColumn(\n",
    "    \"streetadd\",\n",
    "    when(\n",
    "        ~col(\"effend\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\") & (col(\"customerid_stripped\") != \"\"),\n",
    "        concat_ws(\" \", col(\"effend\"), col(\"customerid_stripped\"))\n",
    "    ).otherwise(col(\"streetadd\"))\n",
    ")\n",
    "# Keep only the valid customerid pattern in customerid\n",
    "invalid_customerid_df = invalid_customerid_df.withColumn(\n",
    "    \"customerid\",\n",
    "    regexp_extract(col(\"customerid\"), r\"([A-Za-z]{2}-\\d+)\", 1)\n",
    ")\n",
    "# Drop the customerid_stripped column\n",
    "invalid_customerid_df = invalid_customerid_df.drop(\"customerid_stripped\")\n",
    "# Show the final DataFrame with invalid customerid corrections\n",
    "invalid_customerid_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipmlemented the same logic on the original DataFrame\n",
    "shipping_address_df = shipping_address_df.withColumn(\n",
    "    \"customerid_stripped\",\n",
    "    regexp_extract(col(\"customerid\"), r\"^[A-Za-z]{2}-\\d+(.*)$\", 1)\n",
    ")\n",
    "shipping_address_df = shipping_address_df.withColumn(\n",
    "    \"city\",\n",
    "    when(col(\"city\").isNull() & (col(\"customerid_stripped\") != \"\"), col(\"customerid_stripped\")).otherwise(col(\"city\"))\n",
    ") \n",
    "shipping_address_df = shipping_address_df.withColumn(\n",
    "    \"streetadd\",\n",
    "    when(\n",
    "        ~col(\"effend\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\") & (col(\"customerid_stripped\") != \"\"),\n",
    "        concat_ws(\" \", col(\"effend\"), col(\"customerid_stripped\"))\n",
    "    ).otherwise(col(\"streetadd\"))\n",
    ")\n",
    "shipping_address_df = shipping_address_df.withColumn(\n",
    "    \"customerid\",\n",
    "    regexp_extract(col(\"customerid\"), r\"([A-Za-z]{2}-\\d+)\", 1)\n",
    ")\n",
    "shipping_address_df = shipping_address_df.drop(\"customerid_stripped\")\n",
    "shipping_address_df.show(truncate=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for effstart  that is NOT in the required format\n",
    "invalid_effend_df = shipping_address_df.filter(~col(\"effend\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\"))\n",
    "invalid_effend_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Findings: Invalid `effend` Field Analysis\n",
    "\n",
    "After filtering the `effend` column to identify values that do not match the expected date format (`DD/MM/YYYY`), it was observed that the resulting records correspond to those previously identified as having dirty or malformed `customerid` values.\n",
    "\n",
    "#### Key Observations\n",
    "- The records with invalid `effend` values are the same as those affected by earlier data misalignment, where values were shifted between columns (notably, `customerid`, `city`, and `streetadd`).\n",
    "- In these cases, the dirty value found in `effend` is actually the value that originally belonged in the `streetadd` column. This value has already been moved to its correct place during the previous transformation and cleaning steps for `customerid`.\n",
    "\n",
    "#### Cleaning Decision\n",
    "- Since the dirty records in `effend` are remnants of the earlier misalignment and have already been addressed, all such invalid `effend` values will be set to `null`.\n",
    "- This ensures that the `effend` column contains only valid date values or is left empty (null) where appropriate, maintaining data integrity and consistency.\n",
    "\n",
    "#### Impact\n",
    "- This approach prevents the propagation of misaligned or duplicate data in the `effend` column.\n",
    "- The dataset is now more reliable for downstream processing and analysis, with all date fields properly validated and cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the logic explainmed above to clean effend column\n",
    "invalid_effend_df = invalid_effend_df.withColumn(\n",
    "    \"effend\",\n",
    "    when(~col(\"effend\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\"), lit(None)).otherwise(col(\"effend\")))\n",
    "invalid_effend_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing the same logic on the original DataFrame     \n",
    "shipping_address_df = shipping_address_df.withColumn(\n",
    "    \"effend\",\n",
    "    when(~col(\"effend\").rlike(\"^\\d{2}/\\d{2}/\\d{4}$\"), lit(None)).otherwise(col(\"effend\"))       \n",
    ")\n",
    "shipping_address_df.show(truncate=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing all the values in the 'country_new' column\n",
    "print(\"Distinct values in 'country_new':\")\n",
    "for row in shipping_address_df.select('country_new').distinct().collect():\n",
    "    print(row['country_new'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Findings and Recommendations: Country Column Analysis\n",
    "\n",
    "#### Findings\n",
    "\n",
    "- The analysis of the `country` column revealed significant inconsistencies in how country names are recorded. Examples include:\n",
    "  - Multiple variants for the same country, such as:  \n",
    "    - United States, US, USA, United States of America  \n",
    "    - Alger, Algeria  \n",
    "    - Republic of the Congo, Democratic Republic of the Congo  \n",
    "    - NZ, New Zealand  \n",
    "    - UK, United Kingdom\n",
    "  - Some entries contain extraneous characters preceding the country name, for example: `\"\"Australia`.\n",
    "\n",
    "#### Remediation Plan\n",
    "\n",
    "- **Standardization:**  \n",
    "  All country name variants will be consolidated into a single, standardized form for each country.\n",
    "- **Cleaning:**  \n",
    "  Any leading or extraneous characters before the country name will be stripped to ensure clean and consistent entries.\n",
    "\n",
    "#### Recommendations for Future Data Collection\n",
    "\n",
    "- **Lookup Table:**  \n",
    "  Instead of manually reviewing distinct country values, a lookup table (mapping known variants to standardized names) should be used to efficiently identify and correct inconsistencies.\n",
    "- **Data Entry Controls:**  \n",
    "  To prevent such inconsistencies in production, implement data validation measures such as providing a predefined list of countries for users to select from, rather than allowing free-text entry.\n",
    "\n",
    "This approach will improve data quality, facilitate accurate analysis, and reduce the risk of similar issues in future data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping as a CASE WHEN chain\n",
    "shipping_address_df = shipping_address_df.withColumn(\n",
    "    \"country_new\",\n",
    "    trim(regexp_replace(col(\"country_new\"), r\"^[^A-Za-z]*\", \"\")))\n",
    "\n",
    "shipping_address_df = shipping_address_df.withColumn(\n",
    "    \"country_new\",\n",
    "    when(col(\"country_new\").isin(\"US\", \"USA\", \"United States\"), \"United States of America\")\n",
    "    .when(col(\"country_new\") == \"Alger\", \"Algeria\")\n",
    "    .when(col(\"country_new\").isin(\"Republic of the Congo\", \"Congo\"), \"Democratic Republic of the Congo\")\n",
    "    .when(col(\"country_new\") == \"NZ\", \"New Zealand\")\n",
    "    .when(col(\"country_new\") == \"UK\", \"United Kingdom\")\n",
    "    .otherwise(col(\"country_new\"))\n",
    ")\n",
    "\n",
    "# Show the cleaned DataFrame        \n",
    "shipping_address_df.select(\"country_new\").distinct().show(150,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Product data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading prodcut data\n",
    "\n",
    "product_file_path = \"data/product.json\"\n",
    "product_single_json_df = spark.read.option(\"multiLine\", \"true\").json(product_file_path)\n",
    "\n",
    "# Show the first few records.\n",
    "product_single_json_df.show(truncate=False)\n",
    "\n",
    "# Print the schema.\n",
    "product_single_json_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "The JSON file is structured as a single JSON object with several keys. Each key appears to hold a dictionary where the keys of that dictionaries are IDs and the values are the corresponding values for that field. To work with the data as individual product records, there's a need to \"flatten\" or \"explode\" these map-columns into rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the struct to JSON and then parse it as a MapType(StringType(), StringType())\n",
    "product_transformed_df = product_single_json_df.select(\n",
    "    from_json(to_json(col(\"Category\")), MapType(StringType(), StringType())).alias(\"CategoryMap\"),\n",
    "    from_json(to_json(col(\"Product_ID\")), MapType(StringType(), StringType())).alias(\"Product_IDMap\"),\n",
    "    from_json(to_json(col(\"Product_Name\")), MapType(StringType(), StringType())).alias(\"Product_NameMap\"),\n",
    "    from_json(to_json(col(\"Sub-Category\")), MapType(StringType(), StringType())).alias(\"Sub_CategoryMap\")\n",
    ")\n",
    "\n",
    "product_transformed_df.printSchema()\n",
    "product_transformed_df.show(truncate=False)\n",
    "\n",
    "# Now, for each map column, we convert map entries into separate rows\n",
    "cat_df = product_transformed_df.select(explode(map_entries(col(\"CategoryMap\"))).alias(\"kv_cat\")) \\\n",
    "            .select(col(\"kv_cat.key\").alias(\"id\"),\n",
    "                    col(\"kv_cat.value\").alias(\"Category\"))\n",
    "\n",
    "pid_df = product_transformed_df.select(explode(map_entries(col(\"Product_IDMap\"))).alias(\"kv_pid\")) \\\n",
    "            .select(col(\"kv_pid.key\").alias(\"id\"),\n",
    "                    col(\"kv_pid.value\").alias(\"Product_ID\"))\n",
    "\n",
    "pname_df = product_transformed_df.select(explode(map_entries(col(\"Product_NameMap\"))).alias(\"kv_pname\")) \\\n",
    "            .select(col(\"kv_pname.key\").alias(\"id\"),\n",
    "                    col(\"kv_pname.value\").alias(\"Product_Name\"))\n",
    "\n",
    "sub_df = product_transformed_df.select(explode(map_entries(col(\"Sub_CategoryMap\"))).alias(\"kv_sub\")) \\\n",
    "            .select(col(\"kv_sub.key\").alias(\"id\"),\n",
    "                    col(\"kv_sub.value\").alias(\"Sub_Category\"))\n",
    "\n",
    "# Join all the exploded DataFrames on the common key \"id\".\n",
    "product_df = cat_df.join(pid_df, \"id\") \\\n",
    "                 .join(pname_df, \"id\") \\\n",
    "                 .join(sub_df, \"id\")\n",
    "\n",
    "product_df.show(truncate=False)\n",
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load region data\n",
    "region_file_path = \"data/regiontxt\"\n",
    "region_df= spark.read.option(\"delimiter\", \"\\t\").option(\"header\", 'true').option(\"inferSchema\", \"true\").csv(region_file_path)\n",
    "region_df.show(truncate=False)\n",
    "region_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load invoice data\n",
    "invoice_file_path = \"data/invoice.xml\"\n",
    "\n",
    "# Read the XML file\n",
    "invoice_df = spark.read.format(\"xml\") \\\n",
    "    .option(\"rowTag\", \"row\") \\\n",
    "    .load(invoice_file_path)\n",
    "\n",
    "# Show DataFrame\n",
    "invoice_df.show(truncate=False)\n",
    "invoice_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Customer data\n",
    "customer_file_path = \"data/cust.xlsx\"\n",
    "customer_pd_df = pd.read_excel(customer_file_path)\n",
    "customer_df = spark.createDataFrame(customer_pd_df)\n",
    "customer_df.show(truncate=False)\n",
    "customer_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing all the values in the 'country_new' column\n",
    "print(\"Distinct values in 'country_new':\")\n",
    "for row in shipping_address_df.select('country_new').distinct().collect():\n",
    "    print(row['country_new'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Findings and Recommendations: Country Column Analysis\n",
    "\n",
    "#### Findings\n",
    "\n",
    "- The analysis of the `country` column revealed significant inconsistencies in how country names are recorded. Examples include:\n",
    "  - Multiple variants for the same country, such as:  \n",
    "    - United States, US, USA, United States of America  \n",
    "    - Alger, Algeria  \n",
    "    - Republic of the Congo, Democratic Republic of the Congo  \n",
    "    - NZ, New Zealand  \n",
    "    - UK, United Kingdom\n",
    "  - Some entries contain extraneous characters preceding the country name, for example: `\"\"Australia`.\n",
    "\n",
    "#### Remediation Plan\n",
    "\n",
    "- **Standardization:**  \n",
    "  All country name variants will be consolidated into a single, standardized form for each country.\n",
    "- **Cleaning:**  \n",
    "  Any leading or extraneous characters before the country name will be stripped to ensure clean and consistent entries.\n",
    "\n",
    "#### Recommendations for Future Data Collection\n",
    "\n",
    "- **Lookup Table:**  \n",
    "  Instead of manually reviewing distinct country values, a lookup table (mapping known variants to standardized names) should be used to efficiently identify and correct inconsistencies.\n",
    "- **Data Entry Controls:**  \n",
    "  To prevent such inconsistencies in production, implement data validation measures such as providing a predefined list of countries for users to select from, rather than allowing free-text entry.\n",
    "\n",
    "This approach will improve data quality, facilitate accurate analysis, and reduce the risk of similar issues in future data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading prodcut data\n",
    "\n",
    "product_file_path = \"data/product.json\"\n",
    "product_single_json_df = spark.read.option(\"multiLine\", \"true\").json(product_file_path)\n",
    "\n",
    "# Show the first few records.\n",
    "product_single_json_df.show(truncate=False)\n",
    "\n",
    "# Print the schema.\n",
    "product_single_json_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "The JSON file is structured as a single JSON object with several keys. Each key appears to hold a dictionary where the keys of that dictionaries are IDs and the values are the corresponding values for that field. To work with the data as individual product records, there's a need to \"flatten\" or \"explode\" these map-columns into rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the struct to JSON and then parse it as a MapType(StringType(), StringType())\n",
    "product_transformed_df = product_single_json_df.select(\n",
    "    from_json(to_json(col(\"Category\")), MapType(StringType(), StringType())).alias(\"CategoryMap\"),\n",
    "    from_json(to_json(col(\"Product_ID\")), MapType(StringType(), StringType())).alias(\"Product_IDMap\"),\n",
    "    from_json(to_json(col(\"Product_Name\")), MapType(StringType(), StringType())).alias(\"Product_NameMap\"),\n",
    "    from_json(to_json(col(\"Sub-Category\")), MapType(StringType(), StringType())).alias(\"Sub_CategoryMap\")\n",
    ")\n",
    "\n",
    "product_transformed_df.printSchema()\n",
    "product_transformed_df.show(truncate=False)\n",
    "\n",
    "# Now, for each map column, we convert map entries into separate rows\n",
    "cat_df = product_transformed_df.select(explode(map_entries(col(\"CategoryMap\"))).alias(\"kv_cat\")) \\\n",
    "            .select(col(\"kv_cat.key\").alias(\"id\"),\n",
    "                    col(\"kv_cat.value\").alias(\"Category\"))\n",
    "\n",
    "pid_df = product_transformed_df.select(explode(map_entries(col(\"Product_IDMap\"))).alias(\"kv_pid\")) \\\n",
    "            .select(col(\"kv_pid.key\").alias(\"id\"),\n",
    "                    col(\"kv_pid.value\").alias(\"Product_ID\"))\n",
    "\n",
    "pname_df = product_transformed_df.select(explode(map_entries(col(\"Product_NameMap\"))).alias(\"kv_pname\")) \\\n",
    "            .select(col(\"kv_pname.key\").alias(\"id\"),\n",
    "                    col(\"kv_pname.value\").alias(\"Product_Name\"))\n",
    "\n",
    "sub_df = product_transformed_df.select(explode(map_entries(col(\"Sub_CategoryMap\"))).alias(\"kv_sub\")) \\\n",
    "            .select(col(\"kv_sub.key\").alias(\"id\"),\n",
    "                    col(\"kv_sub.value\").alias(\"Sub_Category\"))\n",
    "\n",
    "# Join all the exploded DataFrames on the common key \"id\".\n",
    "product_df = cat_df.join(pid_df, \"id\") \\\n",
    "                 .join(pname_df, \"id\") \\\n",
    "                 .join(sub_df, \"id\")\n",
    "\n",
    "product_df.show(truncate=False)\n",
    "product_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load region data\n",
    "region_file_path = \"data/regiontxt\"\n",
    "region_df= spark.read.option(\"delimiter\", \"\\t\").option(\"header\", 'true').option(\"inferSchema\", \"true\").csv(region_file_path)\n",
    "region_df.show(truncate=False)\n",
    "region_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load invoice data\n",
    "invoice_file_path = \"data/invoice.xml\"\n",
    "\n",
    "# Read the XML file\n",
    "invoice_df = spark.read.format(\"xml\") \\\n",
    "    .option(\"rowTag\", \"row\") \\\n",
    "    .load(invoice_file_path)\n",
    "\n",
    "# Show DataFrame\n",
    "invoice_df.show(truncate=False)\n",
    "invoice_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Customer data\n",
    "customer_file_path = \"data/cust.xlsx\"\n",
    "customer_pd_df = pd.read_excel(customer_file_path)\n",
    "customer_df = spark.createDataFrame(customer_pd_df)\n",
    "customer_df.show(truncate=False)\n",
    "customer_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing all the values in the 'country_new' column\n",
    "print(\"Distinct values in 'country_new':\")\n",
    "for row in shipping_address_df.select('country_new').distinct().collect():\n",
    "    print(row['country_new'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
